{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ML model\n",
    "> Make sure that you have sucessfully executed the script `Step_01_DataPreprocessing` and the pickle fileshave been created and they are located inside the `data/` folder relative to this file.\n",
    "\n",
    "The preprocessed pickle files will be loaded and split into a training and a validation set. We will then train a sequential model with the capacitive image matrices as input values and the weight values as excpected ouput values. The model and a log of the training process will be saved to the folder `ModelSnapshots/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras as keras\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, MaxPool2D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set 888460\n",
      "Test data set 16781\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(DATA_PATH + \"data.pkl\")\n",
    "df.Weight = df.Weight.astype(np.float)\n",
    "df.BlobImages = df.BlobImages.apply(lambda x : x.reshape(27, 15, 1))\n",
    "\n",
    "df_train = df[df.Set == \"Train\"]\n",
    "df_test = df[(df.Set == \"Test\") & (df.Noise == 0) & (df.Version == \"Normal\")]\n",
    "\n",
    "# Training set        \n",
    "train_x = df_train.BlobImages.values\n",
    "print(\"Training data set %i\" % len(train_x))\n",
    "train_y = df_train.Weight.values\n",
    "\n",
    "# Test set        \n",
    "test_x = df_test.BlobImages.values\n",
    "print(\"Test data set %i\" % len(test_x))\n",
    "test_y = df_test.Weight.values\n",
    "\n",
    "test_y = test_y.astype('float32')\n",
    "train_y = train_y.astype('float32')\n",
    "\n",
    "train_x = np.concatenate(train_x).reshape(-1, 27, 15, 1) / 255.0\n",
    "test_x = np.concatenate(test_x).reshape(-1, 27, 15, 1) / 255.0\n",
    "\n",
    "test_x = test_x.astype('float32')\n",
    "train_x = train_x.astype('float32')\n",
    "\n",
    "#del df\n",
    "#del df_train\n",
    "#del df_test\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7297e61cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0].reshape(27,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 27, 15, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 15, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 15, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 27, 15, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 7, 48)         13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 7, 48)         192       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 3, 48)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 3, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 864)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               110720    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 142,929\n",
      "Trainable params: 142,705\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Train on 888460 samples, validate on 16781 samples\n",
      "Epoch 1/500\n",
      "888460/888460 [==============================] - 68s 76us/step - loss: 310858.8902 - mean_absolute_error: 464.7164 - rmse: 555.5576 - val_loss: 352715.2709 - val_mean_absolute_error: 452.0774 - val_rmse: 563.7005\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 352715.27093, saving model to ./ModelSnapshots/model-001.h5\n",
      "Epoch 2/500\n",
      "888460/888460 [==============================] - 50s 56us/step - loss: 276958.2211 - mean_absolute_error: 435.1635 - rmse: 525.8286 - val_loss: 241201.5251 - val_mean_absolute_error: 385.5891 - val_rmse: 478.8695\n",
      "\n",
      "Epoch 00002: val_loss improved from 352715.27093 to 241201.52512, saving model to ./ModelSnapshots/model-002.h5\n",
      "Epoch 3/500\n",
      "888460/888460 [==============================] - 49s 55us/step - loss: 268616.3904 - mean_absolute_error: 427.3300 - rmse: 517.8049 - val_loss: 366181.0036 - val_mean_absolute_error: 461.5306 - val_rmse: 572.5628\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 241201.52512\n",
      "Epoch 4/500\n",
      "888460/888460 [==============================] - 48s 55us/step - loss: 263851.6358 - mean_absolute_error: 422.8092 - rmse: 513.1876 - val_loss: 232111.8372 - val_mean_absolute_error: 380.8668 - val_rmse: 471.0093\n",
      "\n",
      "Epoch 00004: val_loss improved from 241201.52512 to 232111.83724, saving model to ./ModelSnapshots/model-004.h5\n",
      "Epoch 5/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 261299.4437 - mean_absolute_error: 420.4588 - rmse: 510.6672 - val_loss: 220112.6781 - val_mean_absolute_error: 378.6527 - val_rmse: 461.0204\n",
      "\n",
      "Epoch 00005: val_loss improved from 232111.83724 to 220112.67806, saving model to ./daModelSnapshots/model-005.h5\n",
      "Epoch 6/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 258710.1631 - mean_absolute_error: 418.0608 - rmse: 508.1127 - val_loss: 238896.9581 - val_mean_absolute_error: 385.9227 - val_rmse: 477.3895\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 220112.67806\n",
      "Epoch 7/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 256634.8989 - mean_absolute_error: 416.0416 - rmse: 506.0737 - val_loss: 240317.8458 - val_mean_absolute_error: 375.9515 - val_rmse: 474.2724\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 220112.67806\n",
      "Epoch 8/500\n",
      "888460/888460 [==============================] - 49s 55us/step - loss: 255426.0079 - mean_absolute_error: 414.9150 - rmse: 504.8565 - val_loss: 229852.5658 - val_mean_absolute_error: 376.2910 - val_rmse: 468.1135\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 220112.67806\n",
      "Epoch 9/500\n",
      "888460/888460 [==============================] - 49s 55us/step - loss: 253380.1166 - mean_absolute_error: 413.1562 - rmse: 502.8252 - val_loss: 223412.2776 - val_mean_absolute_error: 370.9081 - val_rmse: 461.4929\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 220112.67806\n",
      "Epoch 10/500\n",
      "888460/888460 [==============================] - 50s 56us/step - loss: 252286.4577 - mean_absolute_error: 412.1376 - rmse: 501.7153 - val_loss: 228842.5471 - val_mean_absolute_error: 381.9979 - val_rmse: 469.0515\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 220112.67806\n",
      "Epoch 11/500\n",
      "888460/888460 [==============================] - 50s 56us/step - loss: 251427.4453 - mean_absolute_error: 411.2538 - rmse: 500.8578 - val_loss: 228825.1135 - val_mean_absolute_error: 376.9521 - val_rmse: 467.1976\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 220112.67806\n",
      "Epoch 12/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 250170.2149 - mean_absolute_error: 409.9270 - rmse: 499.5866 - val_loss: 237375.6346 - val_mean_absolute_error: 380.4353 - val_rmse: 474.7672\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 220112.67806\n",
      "Epoch 13/500\n",
      "888460/888460 [==============================] - 47s 52us/step - loss: 249377.4198 - mean_absolute_error: 409.4060 - rmse: 498.7887 - val_loss: 231785.3011 - val_mean_absolute_error: 373.6729 - val_rmse: 468.4645\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 220112.67806\n",
      "Epoch 14/500\n",
      "888460/888460 [==============================] - 46s 52us/step - loss: 248901.3332 - mean_absolute_error: 408.9528 - rmse: 498.3179 - val_loss: 225821.2762 - val_mean_absolute_error: 374.6576 - val_rmse: 464.6262\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 220112.67806\n",
      "Epoch 15/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 247857.0303 - mean_absolute_error: 408.0846 - rmse: 497.2522 - val_loss: 231731.3423 - val_mean_absolute_error: 376.3171 - val_rmse: 469.1456\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 220112.67806\n",
      "Epoch 16/500\n",
      "888460/888460 [==============================] - 47s 53us/step - loss: 247279.0847 - mean_absolute_error: 407.5521 - rmse: 496.6773 - val_loss: 226464.3466 - val_mean_absolute_error: 370.8924 - val_rmse: 463.6526\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 220112.67806\n",
      "Epoch 17/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 246411.4405 - mean_absolute_error: 406.6585 - rmse: 495.7990 - val_loss: 227195.1736 - val_mean_absolute_error: 371.9533 - val_rmse: 464.6769\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 220112.67806\n",
      "Epoch 18/500\n",
      "888460/888460 [==============================] - 49s 55us/step - loss: 246675.4739 - mean_absolute_error: 406.8123 - rmse: 496.0625 - val_loss: 233552.1022 - val_mean_absolute_error: 381.4507 - val_rmse: 472.4644\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 220112.67806\n",
      "Epoch 19/500\n",
      "888460/888460 [==============================] - 49s 55us/step - loss: 245570.2488 - mean_absolute_error: 405.8493 - rmse: 494.9548 - val_loss: 224226.1307 - val_mean_absolute_error: 370.2572 - val_rmse: 461.9474\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 220112.67806\n",
      "Epoch 20/500\n",
      "888460/888460 [==============================] - 49s 55us/step - loss: 245862.1426 - mean_absolute_error: 406.1556 - rmse: 495.2497 - val_loss: 232482.3118 - val_mean_absolute_error: 376.3586 - val_rmse: 469.4257\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 220112.67806\n",
      "Epoch 21/500\n",
      "888460/888460 [==============================] - 49s 55us/step - loss: 245129.0243 - mean_absolute_error: 405.4919 - rmse: 494.5147 - val_loss: 232661.3096 - val_mean_absolute_error: 384.4154 - val_rmse: 472.3025\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 220112.67806\n",
      "Epoch 22/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 244387.4287 - mean_absolute_error: 404.7853 - rmse: 493.7741 - val_loss: 223074.7616 - val_mean_absolute_error: 371.1517 - val_rmse: 461.2465\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 220112.67806\n",
      "Epoch 23/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 243757.2331 - mean_absolute_error: 404.2737 - rmse: 493.1385 - val_loss: 229282.2099 - val_mean_absolute_error: 389.7778 - val_rmse: 470.8686\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 220112.67806\n",
      "Epoch 24/500\n",
      "888460/888460 [==============================] - 48s 54us/step - loss: 243200.7217 - mean_absolute_error: 403.8060 - rmse: 492.5774 - val_loss: 231527.7958 - val_mean_absolute_error: 385.0634 - val_rmse: 471.8458\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 220112.67806\n",
      "Epoch 26/500\n",
      "327500/888460 [==========>...................] - ETA: 30s - loss: 242480.9098 - mean_absolute_error: 403.3515 - rmse: 491.8288"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):    \n",
    "    # regulate gpu usage\n",
    "    config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)#, device_count = {'GPU' : 4})\n",
    "    config.gpu_options.allow_growth=True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction=0.3\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    with tf.Session(config=config):\n",
    "        model = Sequential()\n",
    "        MODEL_NAME_PREFIX = \"model\"\n",
    "\n",
    "        # First layer of convolution and max pooling\n",
    "        model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (27,15,1)))\n",
    "        model.add(BatchNormalization(axis=-1))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'))\n",
    "        model.add(BatchNormalization(axis=-1)) \n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "        model.add(Conv2D(filters = 48, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'))        \n",
    "        model.add(BatchNormalization(axis=-1))\n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, kernel_regularizer=keras.regularizers.L1L2(0.02, 0.15)))\n",
    "        model.add(keras.layers.advanced_activations.LeakyReLU())\n",
    "        #model.add(keras.layers.ReLU())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(64, kernel_regularizer=keras.regularizers.L1L2(0.02, 0.15)))\n",
    "                        #activity_regularizer=keras.regularizers.l2(0.001)))\n",
    "        #model.add(keras.layers.ReLU())\n",
    "        model.add(keras.layers.advanced_activations.LeakyReLU())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                    patience=10, \n",
    "                                                    verbose=1, \n",
    "                                                    factor=0.9, \n",
    "                                                    min_lr=0.00001)\n",
    "        def rmse(y_true, y_pred):\n",
    "                return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "        #RMSprop optimizer\n",
    "        optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "        model.compile(loss='mse', optimizer=optimizer, metrics=['mae', rmse])\n",
    "\n",
    "\n",
    "        readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "        outFile = MODEL_NAME_PREFIX + \n",
    "        print (outFile)\n",
    "        csv_logger = CSVLogger(MODEL_PATH + '/' + outFile)\n",
    "        tensorflowFolder = \"/srv/share/tensorboardfiles/\" + outFile\n",
    "        tfbCallback = keras.callbacks.TensorBoard(log_dir=tensorflowFolder, histogram_freq=0,  \n",
    "                  write_graph=True, write_images=True)\n",
    "\n",
    "        checkpointCallback = ModelCheckpoint(\"./ModelSnapshots/\" + outFile + '-{epoch:03d}.h5',\n",
    "                                             verbose=1, monitor='val_loss', save_best_only=True, mode='auto')  \n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        # Train data\n",
    "        model.fit(train_x, train_y,\n",
    "                           batch_size=500,\n",
    "                           epochs=500,\n",
    "                           verbose=1,\n",
    "                           validation_data=(test_x, test_y),\n",
    "                           callbacks=[learning_rate_reduction, csv_logger, tfbCallback, checkpointCallback])\n",
    "        \n",
    "        model.save(\"./ModelSnapshots/\" + outFile + '-500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
